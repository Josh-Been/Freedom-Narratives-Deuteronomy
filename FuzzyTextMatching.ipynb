{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fuzzy",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josh-Been/Freedom-Narratives-Deuteronomy/blob/master/fuzzy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNNLZXbDKhyY",
        "colab_type": "text"
      },
      "source": [
        "[PPTX](https://baylor.box.com/s/v31v722ueyvag6l8hnvi7nii02pes57z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z5_OWEVKzX-",
        "colab_type": "text"
      },
      "source": [
        "Specify Input Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6eMfWXfKa8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import output\n",
        "doc_files={}\n",
        "\n",
        "def read_text(type):\n",
        "    # Browse/Upload File\n",
        "    up=files.upload()\n",
        "    # File passed to variable doc\n",
        "    doc=next(iter(up))\n",
        "    if type==1:\n",
        "        try:\n",
        "            f=open(doc,'r', encoding='utf-8-sig')\n",
        "            txt=f.read()\n",
        "            f.close()\n",
        "        except:\n",
        "            f=open(doc,'r', encoding='ISO-8859-1')\n",
        "            txt=f.read()\n",
        "            f.close()\n",
        "        return doc, txt\n",
        "    else:\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(doc, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        for txt in zip_ref.namelist():\n",
        "            if txt.endswith('.txt'):\n",
        "                if txt.endswith('.txt'):\n",
        "                    try:\n",
        "                        f=open(txt, 'r', encoding='utf-8-sig')\n",
        "                        doc_files[txt]=f.read()\n",
        "                        f.close()\n",
        "                    except:\n",
        "                        f=open(txt, 'r', encoding='ISO-8859-1')\n",
        "                        doc_files[txt]=f.read()\n",
        "                        f.close()\n",
        "        return doc_files\n",
        "\n",
        "print('(1) I want to upload a source and target text files to compare.')\n",
        "print('(2) I want to upload a single zip file to comnpare all archived text files')\n",
        "file_choice=input('Enter 1 or 2\\n')\n",
        "\n",
        "if file_choice=='1':\n",
        "    print('First, browse for source text file')\n",
        "    source, source_text=read_text(1)\n",
        "    print('\\nSecond, browse for source text file')\n",
        "    target, target_text=read_text(1)\n",
        "    doc_files[source]=source_text\n",
        "    doc_files[target]=target_text\n",
        "elif file_choice=='2':\n",
        "    print('Browse for .zip file\\nMake sure only text files are archived')\n",
        "    doc_files=read_text(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZDsMVlcPvdR",
        "colab_type": "text"
      },
      "source": [
        "Specify:\n",
        "\n",
        "\n",
        "1.   Ngram Size\n",
        "2.   Fuzzy Method\n",
        "3.   Minimuim Match Score\n",
        "4.   Speed vs. Accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5JCn39PyWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "ngram_size=10\n",
        "fuzzy_method='token sort ratio'  #[ratio, partial ratio, token sort ratio, token set ratio, exact]\n",
        "minimum_score=70\n",
        "speed=3    # 1 is slowest, represents the nth ngrams checked in the target file. 1 is every ngram. 2 is every other ngram. 3 is every third ngram.\n",
        "#################################\n",
        "\n",
        "if fuzzy_method!='exact':\n",
        "    !pip install python-Levenshtein\n",
        "    from google.colab import output\n",
        "    try:\n",
        "        from fuzzywuzzy import fuzz\n",
        "    except:\n",
        "        !pip install fuzzywuzzy\n",
        "        from fuzzywuzzy import fuzz\n",
        "output.clear()\n",
        "\n",
        "import csv, re, math\n",
        "\n",
        "def all_pairs(source):\n",
        "    result = []\n",
        "    for p1 in range(len(source)):\n",
        "        for p2 in range(p1+1,len(source)):\n",
        "            result.append([source[p1],source[p2]])\n",
        "    return result\n",
        "\n",
        "def strip_non_ascii(cleanme):\n",
        "    cleanme = re.sub(r'[^\\w\\s]','',cleanme)\n",
        "    cleanme = cleanme.replace('\\n',' ')\n",
        "    cleanme = cleanme.replace('\\r',' ')\n",
        "    cleanme = cleanme.lower()\n",
        "    cleanme = ' '.join([word for word in cleanme.split() if not 'page' in word and not word.isdigit()])\n",
        "    return ''.join([i if ord(i) < 128 else ' ' for i in cleanme])\n",
        "\n",
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input)-n+1):\n",
        "        output.append(' '.join(input[i:i+n]))\n",
        "    return output\n",
        "\n",
        "def roundup(x):\n",
        "    return int(math.ceil(x / 100.0)) * 100\n",
        "\n",
        "matches=['DOC1_TITLE,DOC1_NGRAM,DOC2_TITLE,DOC2_NGRAM,SCORE,MATCH_METHOD\\n']\n",
        "used_keys=[]\n",
        "\n",
        "for x in all_pairs(list(doc_files.keys())):\n",
        "    source_ngrams=(ngrams(strip_non_ascii(doc_files[x[0]]),ngram_size))\n",
        "    target_ngrams=(ngrams(strip_non_ascii(doc_files[x[1]]),ngram_size))\n",
        "    i=0\n",
        "    skip=0\n",
        "    for s in source_ngrams:\n",
        "        skip=skip-1\n",
        "        i+=1\n",
        "        if i==1 or i%100==0:\n",
        "            print('\\n'+x[0]+'\\n'+x[1]+'\\nChecking Ngram '+str(i)+'-'+str(roundup(i+99))+'/'+str(len(source_ngrams)))\n",
        "        if skip <=0:\n",
        "            for target_skip in range(0,len(target_ngrams)):\n",
        "                if target_skip==0 or target_skip%speed==0:\n",
        "                    measure=0\n",
        "                    if fuzzy_method=='ratio':\n",
        "                        measure=fuzz.ratio(s,target_ngrams[target_skip])\n",
        "                    elif fuzzy_method=='partial ratio':\n",
        "                        measure=fuzz.partial_ratio(s,target_ngrams[target_skip])\n",
        "                    elif fuzzy_method=='token sort ratio':\n",
        "                        measure=fuzz.token_sort_ratio(s,target_ngrams[target_skip])\n",
        "                    elif fuzzy_method=='token set ratio':\n",
        "                        measure=fuzz.token_set_ratio(s,target_ngrams[target_skip])\n",
        "                    ### Adding exact\n",
        "                    elif s==target_ngrams[target_skip]:\n",
        "                        measure=100\n",
        "                    ###\n",
        "                    if measure>=minimum_score:\n",
        "                        matches.append(x[0].replace(',','')+','+s.replace(',','')+','+x[1].replace(',','')+','+target_ngrams[target_skip].replace(',','')+','+str(measure)+','+fuzzy_method+'\\n')\n",
        "                        print(measure, '  >>  ', s, '|', target_ngrams[target_skip])\n",
        "                        skip=ngram_size/2 \n",
        "\n",
        "f=open('similarities.csv','w', encoding='utf-8-sig')\n",
        "for match in matches:\n",
        "    f.write(match)\n",
        "f.close()\n",
        "\n",
        "print('\\nCompleted!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
