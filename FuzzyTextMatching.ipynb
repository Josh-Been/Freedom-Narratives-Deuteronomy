{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fuzzy",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josh-Been/SBL-Freedom-Deuteronomy/blob/master/FuzzyTextMatching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNNLZXbDKhyY",
        "colab_type": "text"
      },
      "source": [
        "[PPTX](https://baylor.box.com/s/v31v722ueyvag6l8hnvi7nii02pes57z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z5_OWEVKzX-",
        "colab_type": "text"
      },
      "source": [
        "Specify Source File\n",
        "### Must be a .TXT text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6eMfWXfKa8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import output\n",
        "\n",
        "# Browse/Upload File\n",
        "up=files.upload()\n",
        "# File passed to variable doc\n",
        "sourcedoc=next(iter(up))\n",
        "\n",
        "try:\n",
        "    f=open(sourcedoc,'r', encoding='utf-8-sig')\n",
        "    source=f.read()\n",
        "    f.close()\n",
        "except:\n",
        "    f=open(sourcedoc,'r', encoding='ISO-8859-1')\n",
        "    source=f.read()\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LQ3zYjK5dR",
        "colab_type": "text"
      },
      "source": [
        "Specify Target File\n",
        "### Must be a .TXT text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxR0RWlbK8gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Browse/Upload File\n",
        "up=files.upload()\n",
        "# File passed to variable doc\n",
        "targetdoc=next(iter(up))\n",
        "\n",
        "try:\n",
        "    f=open(targetdoc,'r', encoding='utf-8-sig')\n",
        "    target=f.read()\n",
        "    f.close()\n",
        "except:\n",
        "    f=open(targetdoc,'r', encoding='ISO-8859-1')\n",
        "    target=f.read()\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZDsMVlcPvdR",
        "colab_type": "text"
      },
      "source": [
        "Specify:\n",
        "\n",
        "\n",
        "1.   Ngram Size\n",
        "2.   Fuzzy Method\n",
        "3.   Minimuim Match Score\n",
        "4.   Speed vs. Accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5JCn39PyWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "ngram_size=10\n",
        "fuzzy_method='ratio'  #[ratio, partial ratio, token sort ratio, token set ratio]\n",
        "minimum_score=80\n",
        "speed=3    # 1 is slowest, represents the nth ngrams checked in the target file. 1 is every ngram. 2 is every other ngram. 3 is every third ngram.\n",
        "#################################\n",
        "\n",
        "!pip install python-Levenshtein\n",
        "try:\n",
        "    from fuzzywuzzy import fuzz\n",
        "except:\n",
        "    !pip install fuzzywuzzy\n",
        "    from fuzzywuzzy import fuzz\n",
        "output.clear()\n",
        "\n",
        "import csv, re, math\n",
        "\n",
        "def strip_non_ascii(cleanme):\n",
        "    cleanme = re.sub(r'[^\\w\\s]','',cleanme)\n",
        "    cleanme = cleanme.replace('\\n',' ')\n",
        "    cleanme = cleanme.replace('\\r',' ')\n",
        "    cleanme = cleanme.lower()\n",
        "    cleanme = ' '.join([word for word in cleanme.split() if not 'page' in word and not word.isdigit()])\n",
        "    return ''.join([i if ord(i) < 128 else ' ' for i in cleanme])\n",
        "\n",
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input)-n+1):\n",
        "        output.append(' '.join(input[i:i+n]))\n",
        "    return output\n",
        "\n",
        "def roundup(x):\n",
        "    return int(math.ceil(x / 100.0)) * 100\n",
        "\n",
        "source_ngrams=(ngrams(strip_non_ascii(source),ngram_size))\n",
        "target_ngrams=(ngrams(strip_non_ascii(target),ngram_size))\n",
        "\n",
        "matches=[sourcedoc.replace(',','')+','+targetdoc.replace(',','')+',Similarity\\n']\n",
        "i=0\n",
        "skip=0\n",
        "\n",
        "for s in source_ngrams:\n",
        "    skip=skip-1\n",
        "    i+=1\n",
        "    if i==1 or i%100==0:\n",
        "        print('Checking Ngram '+str(i)+'-'+str(roundup(i+99))+'/'+str(len(source_ngrams)))\n",
        "    if skip <=0:\n",
        "        for target_skip in range(0,len(target_ngrams)):\n",
        "            if target_skip==0 or target_skip%speed==0:\n",
        "                if fuzzy_method=='ratio':\n",
        "                    measure=fuzz.ratio(s,target_ngrams[target_skip])\n",
        "                elif fuzzy_method=='partial ratio':\n",
        "                    measure=fuzz.partial_ratio(s,target_ngrams[target_skip])\n",
        "                elif fuzzy_method=='token sort ratio':\n",
        "                    measure=fuzz.token_sort_ratio(s,target_ngrams[target_skip])\n",
        "                elif fuzzy_method=='token set ratio':\n",
        "                    measure=fuzz.token_set_ratio(s,target_ngrams[target_skip])\n",
        "                if measure>=minimum_score:\n",
        "                    matches.append(s+','+target_ngrams[target_skip]+','+str(measure)+'\\n')\n",
        "                    print(measure, '  >>  ', s, '|', target_ngrams[target_skip])\n",
        "                    skip=ngram_size/2\n",
        "\n",
        "f=open('similarities.csv','w', encoding='utf-8-sig')\n",
        "for match in matches:\n",
        "    f.write(match)\n",
        "f.close()\n",
        "\n",
        "print('\\nCompleted!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}